import os
import logging
from typing import Any, Mapping, Optional, List

import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted
from langchain.llms.base import LLM
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# ë¡œê·¸ ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class GeminiLLM(LLM):
    """
    Google Gemini APIë¥¼ LangChain LLM ì¸í„°í˜ì´ìŠ¤ë¡œ ê°ì‹¼ ì»¤ìŠ¤í…€ í´ë˜ìŠ¤
    """

    model_name: str = "gemini-2.0-flash"

    def __init__(self, api_key: str, model_name: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.callbacks = kwargs.get('callbacks', None)
        self.tags = kwargs.get('tags', None)
        self.verbose = kwargs.get('verbose', False)

        if model_name:
            self.model_name = model_name

        # Gemini API êµ¬ì„±
        genai.configure(api_key=api_key)

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """
        LangChain ë‚´ë¶€ì—ì„œ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œ
        í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ Gemini APIë¡œ ì‘ë‹µì„ ìƒì„±
        """
        try:
            model = genai.GenerativeModel(self.model_name)
            response = model.generate_content(prompt)
            if hasattr(response, 'text') and response.text:
                return response.text
            else:
                logger.warning("Gemini API returned empty response")
                return "[ì‘ë‹µ ì—†ìŒ] ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤."
        except ResourceExhausted as e:
            logger.warning(f"Gemini API quota exhausted: {e}")
            return "[í• ë‹¹ëŸ‰ ì´ˆê³¼] ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        except Exception as e:
            logger.error(f"Gemini ìƒì„± ì˜¤ë¥˜: {e}", exc_info=True)
            return f"[LLM í˜¸ì¶œ ì‹¤íŒ¨] {str(e)}"

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"model_name": self.model_name}

    @property
    def _llm_type(self) -> str:
        return "gemini"


# Cross-Encoder ê¸°ë°˜ ë¬¸ì„œ ì¬ì •ë ¬
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

def cross_encoder_rerank(query: str, docs: List[Any], top_k: int = 3) -> List[Any]:
    """
    Cross-Encoder ëª¨ë¸ë¡œ ë¬¸ì„œ relevance ì ìˆ˜ ê³„ì‚° í›„ ì¬ì •ë ¬
    """
    model_name = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)

    pairs = [(query, doc.page_content) for doc in docs]
    inputs = tokenizer.batch_encode_plus(pairs, return_tensors="pt", truncation=True, padding=True)

    with torch.no_grad():
        logits = model(**inputs).logits.squeeze()

    scores = logits.tolist()
    reranked = [doc for _, doc in sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)]
    return reranked[:top_k]


# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt_template = """
ë„ˆëŠ” 'BOAZ'ë¼ëŠ” ì´ë¦„ì˜ ë¹…ë°ì´í„° ì—°í•©ë™ì•„ë¦¬ì— ëŒ€í•´ ì•ˆë‚´í•˜ëŠ” ê³ ë„í™”ëœ ì „ë¬¸ ì±—ë´‡ì´ì•¼.
ì‚¬ìš©ìëŠ” ì´ ë™ì•„ë¦¬ì— ì§„ì§€í•œ ê´€ì‹¬ì„ ê°–ê³  ìˆìœ¼ë©°, ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì‹ ë¢°ë„ ë†’ì€ ì •ë³´ë¥¼ ì›í•´.

ë‹¤ìŒ ì§€ì‹œì‚¬í•­ì„ ë°˜ë“œì‹œ ì§€ì¼œì„œ ë‹µë³€í•´ ì¤˜:

0. ì‚¬ëŒ ì´ë¦„ì€ ì ˆëŒ€ ì œê³µí•˜ì§€ ë§ˆ.
1. "ë³´ì•„ì¦ˆëŠ” ë­í•˜ëŠ” ê³³ì´ì•¼?"ì²˜ëŸ¼ ê´‘ë²”ìœ„í•œ ì§ˆë¬¸ì—ëŠ” í•µì‹¬ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ 300~400ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´.
2. ë³µìˆ˜ì˜ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ìœ¼ë¡œ ì•Œë ¤ì¤˜.
3. ê° í•­ëª©ì€ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´. ë„ˆë¬´ ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ë§ˆ.
4. "ì–´ë–¤ê²Œ ìˆì–´?" ê°™ì€ ì§ˆë¬¸ì—” í‘œ í˜•íƒœì˜ ëª©ë¡ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ë³´ì—¬ì¤˜.
5. "ì–´ë–¤ê²Œ ìˆì—ˆì–´?"ì²˜ëŸ¼ ê³¼ê±°ë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì—ëŠ” ìˆë‹¤/ì—†ë‹¤, ë…„ë„, ê¸°ë³¸ ì •ë³´ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µí•´.
6. "OOì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”" ìš”ì²­ì—ëŠ” ê·¼ê±° ìˆëŠ” ê°„ë‹¨í•œ ì¶”ì²œì„ í•´ì¤˜.
7. ê°œì¸ì •ë³´ ìš”ì²­ ì‹œì—ëŠ” "ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ê±°ì ˆí•´.
8. ë‹µë³€ ë§ˆì§€ë§‰ì—” í•­ìƒ â€œì¶”ê°€ ë¬¸ì˜ê°€ í•„ìš”í•˜ë©´ ì–¸ì œë“  ì•Œë ¤ì£¼ì„¸ìš”.â€ë¼ëŠ” ì¹œê·¼í•œ ë§ˆë¬´ë¦¬ ë©˜íŠ¸ë¥¼ ë„£ì–´.
9. ë™ì•„ë¦¬ì™€ ë¬´ê´€í•œ ì§ˆë¬¸, ë˜ëŠ” ì •ì±…ìƒ ë‹µë³€ ë¶ˆê°€í•œ ë‚´ìš©ì€ "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤."ë¼ê³  ë§í•´ì¤˜.

ğŸ“˜ ê·œì¹™
- ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´.
- ì œê³µëœ ì°¸ì¡° ë¬¸ì„œ(context)ì˜ ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•´ ë‹µë³€í•´.
- ë‚´ìš©ì„ ëª¨ë¥¼ ê²½ìš°, "ì •í™•í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ê³µì‹ í™ˆí˜ì´ì§€ https://www.bigdataboaz.com ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”."ë¼ê³  ì•ˆë‚´í•´.
- ë¯¼ê°í•˜ê±°ë‚˜ ì¶”ë¡ ì„± ë†’ì€ ë‚´ìš©ì€ ì ˆëŒ€ ì‘ì„±í•˜ì§€ ë§ˆ. ì´ê±´ ì •ë§ ì¤‘ìš”í•´!

ì§ˆë¬¸: {question}

ì°¸ì¡° ë¬¸ì„œ: {context}

ë‹µë³€ (í•œêµ­ì–´):
"""

prompt = PromptTemplate(
    input_variables=["question", "context"],
    template=prompt_template,
)


def build_qa_chain_with_rerank(llm: LLM, retriever: Any, top_k: int = 3):
    """
    Cross-Encoder rerankê°€ í†µí•©ëœ LangChain QA ì²´ì¸ êµ¬ì„±
    """
    def rerank_retriever(query: str) -> List[Document]:
        initial_docs = retriever.get_relevant_documents(query)
        return cross_encoder_rerank(query, initial_docs, top_k=top_k)

    # LangChainì˜ RetrievalQA êµ¬ì¡°ë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•
    class CustomQAChain:
        def invoke(self, inputs: dict):
            query = inputs["query"]
            docs = rerank_retriever(query)
            context = "\n\n".join(doc.page_content for doc in docs)
            final_prompt = prompt.format(question=query, context=context)
            answer = llm(final_prompt)
            return {"result": answer, "source_documents": docs}

    return CustomQAChain()


def run_qa_chain(chain, query: str):
    """
    QA ì²´ì¸ì„ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ë° ì°¸ì¡° ë¬¸ì„œë¥¼ ë°˜í™˜
    """
    try:
        if hasattr(chain, 'invoke'):
            result = chain.invoke({"query": query})
        else:
            result = chain({"query": query})
        return result
    except Exception as e:
        logger.error(f"QA Chain ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return {"result": f"[ì‹¤í–‰ ì‹¤íŒ¨] {str(e)}", "source_documents": []}
